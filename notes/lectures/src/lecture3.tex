\section{Overview}

This lecture continues the discussion of floating-point arithmetic, error analysis (especially catastrophic cancellation), and the concepts of backward and forward errors in numerical computations.

\section{Catastrophic Cancellation}

\noindent
\textbf{Key Issue:} Subtracting two nearly equal numbers can cancel out the most significant digits, resulting in a large relative error.

\subsection{Example Problem}

Evaluate
\[
\frac{4}{4 - x} - \frac{4}{4 + x}
\]
for $x$ close to $0$.

\begin{itemize}
  \item \textbf{Method 1: Direct Evaluation.}
  \[
    S_1 = \mathrm{fl}\Bigl(\frac{4}{4 - x}\Bigr) - \mathrm{fl}\Bigl(\frac{4}{4 + x}\Bigr).
  \]
  Each division and subtraction can introduce floating-point errors $(1 + \delta_i)$. When $x$ is small, the two terms are nearly equal and can yield a large relative error.

  \item \textbf{Method 2: Re-arrange Calculation.}
  \[
    \frac{4}{4 - x} - \frac{4}{4 + x}
    = \frac{4(4 + x) - 4(4 - x)}{(4 - x)(4 + x)}
    = \frac{8x}{16 - x^2}.
  \]
  This form is often more accurate for $x$ near $0$, avoiding subtracting nearly equal quantities, and thus reducing the relative error.
\end{itemize}

\subsection{Another Example: Exponential and Logarithms}

Let us consider evaluating \(\displaystyle e^{a+x} - e^a\). If \(x\) is very small, a direct approach might suffer catastrophic cancellation.

\begin{itemize}
  \item \textbf{Method 1: Direct Evaluation.}
  \[
    \mathrm{Output}_1 
    = \mathrm{fl}\bigl(e^{a+x}\bigr) 
      - \mathrm{fl}\bigl(e^a\bigr).
  \]
  Potentially large relative error if \(e^{a+x} \approx e^a\).

  \item \textbf{Method 2: Factor and Re-arrange.}
  \[
    e^{a+x} - e^a 
    = e^a \bigl(e^x - 1\bigr).
  \]
  Then compute \(e^x - 1\) carefully (using series expansions or a special library call) to avoid losing significant digits when \(x\) is small.
\end{itemize}

\section{Backward and Forward Error Review}

Recall from previous lectures:

\[
\text{Relative forward error} 
= \frac{\|\mathrm{fl}(x) - x_{\mathrm{exact}}\|}{\|x_{\mathrm{exact}}\|},
\]
\[
\text{Relative backward error}
= \min\Bigl\{ \epsilon : \mathrm{fl}(x) 
   \text{ is the exact solution of } f(x+\Delta x) 
   \text{ for some } \|\Delta x\| \le \epsilon \|x\|\Bigr\}.
\]

\subsection{Stability Definitions}

\begin{itemize}
    \item \textbf{Backward stable:} If \(\mathrm{fl}(x)\) is the exact solution for a slightly perturbed problem. Formally, \(\exists\,\Delta x\) with \(\|\Delta x\| = O(\varepsilon_{\mathrm{mach}})\,\|x\|\) such that \(F(x+\Delta x) = F_{\mathrm{computed}}\).
    \item \textbf{Numerically stable:} If \(\mathrm{fl}(x)\) is close in forward error to the true solution. Often implied by backward stability when the problem is well-conditioned.
\end{itemize}

Examples:
\begin{itemize}
    \item \textbf{Inner product} is backward stable under standard floating-point summation.
    \item \textbf{Outer product} is also backward stable (exercise).
\end{itemize}

\section{Condition Number}

\noindent
The condition number \(\kappa\) measures how input perturbations affect the output. For a function \(f(\cdot)\) (differentiable),
\[
\kappa = \sup_{\Delta x} \frac{\|Df(x)\,\Delta x\|}{\|f(x)\|} 
         \cdot \frac{\|x\|}{\|\Delta x\|},
\]
or more simply (for linear problems \(f(x)=Ax\)):
\[
\kappa(A) = \|A\| \,\|A^{-1}\|.
\]
A high condition number indicates an ill-conditioned (sensitive) problem where small changes in input lead to large changes in output.

\section{Vector and Matrix Norms}

We use norms to measure errors, especially \emph{subordinate norms}:
\begin{itemize}
    \item For \(x \in \mathbb{R}^n\):
      \(\|x\|_1 = \sum_i |x_i|,\; 
        \|x\|_2 = \sqrt{\sum_i |x_i|^2},\; 
        \|x\|_\infty = \max_i |x_i|\).
    \item For \(A \in \mathbb{R}^{m \times n}\) in a consistent norm,
      \(\|A\| = \sup_{x \neq 0} \frac{\|Ax\|}{\|x\|}.\)
\end{itemize}

\subsection{Examples of Matrix Norms}
\begin{itemize}
    \item \(\|A\|_1 = \max_{j} \sum_i |a_{ij}|\) (max column sum).
    \item \(\|A\|_\infty = \max_{i} \sum_j |a_{ij}|\) (max row sum).
    \item \(\|A\|_2 = \sqrt{\lambda_{\max}(A^T A)}\) (spectral norm).
    \item \(\|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}\) (Frobenius norm).
\end{itemize}

\section{Combining Condition and Stability}

\subsection{Forward Error Bound}

For a backward stable algorithm on a well-conditioned problem,
\[
\|\mathrm{fl}(x) - x_{\mathrm{exact}}\| 
\;\le\; \kappa \,\times\, (\text{small backward error}).
\]
Hence, \emph{backward error} plus \emph{condition number} gives a \emph{forward error} estimate.

\section{Differentiable Functions \& Jacobians}

If \(f:\mathbb{R}^n \to \mathbb{R}^n\) is differentiable, with Jacobian \(Df(x)\):
\[
f(x + \Delta x) \approx f(x) + Df(x)\,\Delta x.
\]
A small \(\|\Delta x\|\) can produce a large \(\|Df(x)\,\Delta x\|\) if \(\|Df(x)\|\) is large, implying ill-conditioning.

\section{Summary}

\begin{itemize}
    \item Catastrophic cancellation arises when subtracting nearly equal numbers.
    \item Rewriting expressions (e.g., factoring common terms) mitigates significant digit loss.
    \item Backward stability: the computed result solves a slightly perturbed original problem \emph{exactly}.
    \item Forward error depends on both backward error and problem conditioning.
    \item Norms and condition numbers help quantify sensitivity to small input changes.
\end{itemize}
