\section{Fundamentals of Numerical Analysis}

\textbf{Today:} Floating-point arithmetic and backward error analysis.  
(\textit{Also a brief review/overview of numerical linear algebra (NLA).})

\subsection*{Motivation}

To do linear algebra on computers, we first must understand how numbers are stored and arithmetic is done. The challenge:
\[
\mathbb{R} \text{ is unbounded and (conceptually) continuous,}
\]
while computers are \emph{discrete} and have \emph{finite memory}.

\section*{Fixed-Point Numbers (Idea 1)}

We discretize $\mathbb{R}$ into evenly spaced points:
\[
\cdots, -2h, -h, 0, h, 2h, \cdots.
\]
Then map a real number $x$ to the nearest such point.

\begin{itemize}
    \item Pros: Absolute rounding error can be small for numbers within range.
    \item Cons: Not suitable for representing very large or very small numbers; overflow/underflow can occur easily.
\end{itemize}

\section*{Floating-Point Numbers (Idea 2)}

Mimic scientific notation:
\[
x \;=\; \pm \, m \times B^e,
\]
where $B$ is the \emph{base} (commonly $2$ on binary computers), $m$ (mantissa) satisfies $1 \le m < B$ (for normalized representation), and $e$ is an integer in a finite range $[e_{\min},\, e_{\max}]$.

\subsection*{IEEE Floating-Point (Examples)}

\begin{itemize}
    \item \textbf{Single Precision (float32):} 1 sign bit, 8 exponent bits, 23 fraction bits.
    \item \textbf{Double Precision (float64):} 1 sign bit, 11 exponent bits, 52 fraction bits.
\end{itemize}

\noindent
Hence, floating-point numbers are \emph{not} equally spaced; the gap between representable numbers depends on $e$.

\section*{Machine Epsilon \texorpdfstring{($\varepsilon_{\mathrm{mach}}$)}{}}

Define $\varepsilon_{\mathrm{mach}}$ to be (roughly) the distance between $1.0$ and the next representable number in the floating-point system. This measures the scale of rounding errors. For binary IEEE floating point:
\[
\varepsilon_{\mathrm{mach}} = 2^{-t+1},
\]
where $t$ is the number of fraction bits (if normalized).

\section*{Floating-Point Arithmetic Model}

For an operation $*$ in $\{+,\,-,\,\times,\,\div\}$,
\[
x \;*_{\mathrm{fl}}\; y \;=\; \mathrm{fl}(x * y) \;=\; (x * y)\,\bigl(1 + \delta\bigr),
\quad
|\delta| \le \varepsilon_{\mathrm{mach}}.
\]
This standard model implies:
\begin{itemize}
    \item Rounding is \emph{non-associative}:
    \[
      (x + y) + z \;\neq\; x + (y + z).
    \]
    \item Catastrophic cancellation occurs if $x \approx y$ and we compute $x - y$.
\end{itemize}

\section*{Rounding Error Analysis}

\subsection*{Example: Inner Product}

Consider $u^T v = \sum_{i=1}^n u_i v_i$. A \emph{naive summation} does
\[
S_1 = \mathrm{fl}(u_1 v_1), \quad
S_2 = \mathrm{fl}\bigl(S_1 + \mathrm{fl}(u_2 v_2)\bigr), \;\dots,\;
S_n = \mathrm{fl}\bigl(S_{n-1} + \mathrm{fl}(u_n v_n)\bigr).
\]
Each operation introduces a factor of $(1 + \delta_i)$, with $|\delta_i| \le \varepsilon_{\mathrm{mach}}$. Summing $n$ times yields a worst-case error on the order of $n\,\varepsilon_{\mathrm{mach}}$.

\begin{itemize}
    \item By a standard lemma, the final result satisfies:
    \[
      S_n = \bigl(u^T v\bigr)\,\bigl(1 + \theta_n\bigr),
      \quad
      |\theta_n| \le (n-1)\,\varepsilon_{\mathrm{mach}}.
    \]
    \item \textbf{Alternative Summation Methods (e.g., Kahan summation)} can reduce error.
\end{itemize}

\section*{Backward \& Forward Errors}

\begin{itemize}
    \item \textbf{Backward Error} asks: does our computed result $\widetilde{x}$ solve \emph{exactly} a slightly perturbed problem $f(x + \Delta x) = 0$?
    \item \textbf{Forward Error} compares $\widetilde{x}$ directly to the exact solution $x^*$ of the original problem.
\end{itemize}

\subsubsection*{Definitions}
\[
\text{Relative forward error} = \frac{\|\widetilde{x} - x^*\|}{\|x^*\|}, 
\quad
\text{Relative backward error} 
= \min \bigl\{\epsilon : \widetilde{x} \text{ solves } f(x+\Delta x)\text{ for }\|\Delta x\|\le \epsilon \|x\|\bigr\}.
\]

\noindent
\textbf{Backward Stable:} $\widetilde{x}$ is the exact solution to a perturbed problem with $\|\Delta x\|$ on the order of $\varepsilon_{\mathrm{mach}}$. If the problem is well-conditioned, backward stability implies small forward error.

\section*{Examples}

\begin{itemize}
    \item \textbf{Inner product} is backward stable:
    \[
      \mathrm{fl}(u^T v) = u^T v + \Delta (u^T v),
      \quad
      \|\Delta (u^T v)\| \le \text{(small)} \times \|u^T v\|.
    \]
    \item \textbf{Outer product} is also backward stable (exercise).
\end{itemize}

\section*{Why Backward Stability Implies Numerical Stability}

If the backward error is small (i.e., the algorithm solves a nearby problem exactly) and $f$ is well-conditioned, the forward error is also small. 

\begin{itemize}
    \item \emph{Forward Error} $\;\le\;\; (\text{Condition Number}) \;\times\; (\text{Backward Error}).$
\end{itemize}

\section*{Condition Number (Refresher)}

For a function $f$ or a matrix equation $Ax = b$,
\[
\kappa \;=\; \|A\|\;\|A^{-1}\|,
\]
using a consistent norm. A large $\kappa$ indicates sensitivity (ill-conditioning): small input changes cause large output changes.

\section*{Summary of Lecture 2}

\begin{itemize}
    \item Floating-point arithmetic has finite precision and introduces rounding errors.
    \item Catastrophic cancellation arises from subtracting nearly equal numbers.
    \item Backward error analysis checks whether the computed solution solves a \emph{slightly} perturbed problem exactly.
    \item Well-conditioned problems + backward-stable algorithms $\implies$ small forward error.
\end{itemize}
