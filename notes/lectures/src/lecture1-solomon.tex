\section*{Error Analysis}

\begin{itemize}
  \item \textbf{Forward Error} $\rightarrow$ Error that we cannot measure. ($x - \hat{x}$)
  \item \textbf{Backward Error} $\rightarrow$ Error we can measure. ($f(x) - f(\hat{x})$)
\end{itemize}

When we can bound our error (forward error) using backward error, we have a well-conditioned problem.

\subsection*{Conditioning}

Condition value: $\approx$ $slope^{-1}$

Hence, the higher the slope of a function, the better our problem conditioning. Meaning, less is better.

\section*{An Interesting Problem}

An interesting problem is numerical analysis is when we want to calculate sum of some variables with high range of variation. 
In such a case it can happen that we get to a different result depending on whether we start from front or back of the array.
This can be due to rounding characteristic of floating value in computer. The solutions for this problem are very interesting.
\begin{enumerate}
  \item Firstly, one can think of sorting the arrays first and then summing them up starting from smaller values so that their values accumulate
  and do not get rounded off.
  \item Another options is to use the resources we have in computer i.e. computation power and memory. In this case we use memory to save a running
  value and its respective error. This is the basis for the Kahan summation algorithm.
\end{enumerate}
